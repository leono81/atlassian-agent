import streamlit as st
import logfire
from conversational_jira_confluence_agent.config import settings # Para acceder a las configuraciones

# Configurar Logfire (Tarea 0.3)
# Si el token est치 en .env, se usar치 para enviar a la plataforma.
# Si no, funcionar치 localmente (necesitar치s `logfire.instrument_pydantic_ai()` etc. m치s adelante)
logfire.configure(
    token=settings.LOGFIRE_TOKEN, # Puede ser None si no se env칤a a la plataforma
    send_to_logfire="if-token-present",
    # Podr칤as a침adir `service_name` y `service_version` aqu칤.
)
# logfire.instrument_pydantic_ai() # Se llamar치 cuando el agente est칠 listo

st.set_page_config(page_title="Agente Jira/Confluence", layout="wide")

st.title("游뱄 Agente Conversacional para Jira y Confluence")

st.write(f"Usando el modelo PydanticAI: {settings.PYDANTIC_AI_MODEL}")
st.write(f"Conectado a Jira: {settings.JIRA_URL}")
st.write(f"Conectado a Confluence: {settings.CONFLUENCE_URL}")

if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("쮺칩mo puedo ayudarte con Jira o Confluence?"):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # Aqu칤 ir치 la l칩gica para llamar al agente
    with st.chat_message("assistant"):
        response_placeholder = st.empty()
        # Simulaci칩n de respuesta del agente
        simulated_response = f"Procesando tu solicitud: '{prompt}'... (L칩gica del agente a칰n no implementada)"
        response_placeholder.markdown(simulated_response)
    st.session_state.messages.append({"role": "assistant", "content": simulated_response})

st.sidebar.info("Este es un agente en desarrollo.")